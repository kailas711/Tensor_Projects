{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe6d2f9",
   "metadata": {},
   "source": [
    "# Exercises \n",
    "Take the lessons from the last two notebooks and apply them to the following exercies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8fbc82",
   "metadata": {},
   "source": [
    "## Corpus \n",
    "For the following exercies, use this paragraph as the corups:\n",
    "\n",
    "```The use of natural language processing (NLP) is rapidly becoming an integral part of many businesses. NLP is the process of deriving meaning and understanding from text, such as analyzing the sentiment of a customer review or determining the intent of an email response. For instance, a company may use NLP to extract meaningful insights from customer feedback and influence product design decisions. By leveraging the power of NLP, organizations can gain valuable insights into customer thoughts, behaviors, patterns, and preferences. Consequently, NLP is being used in many industries to uncover opportunities for better customer experiences, increased efficiency in operations, improved decision-making, and advanced analytics.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6f6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"The use of natural language processing (NLP) is rapidly becoming an integral part of many businesses.\n",
    "NLP is the process of deriving meaning and understanding from text, such as analyzing the sentiment of a customer \n",
    "review or determining the intent of an email response. For instance, a company may use NLP to extract meaningful \n",
    "insights from customer feedback and influence product design decisions. By leveraging the power of NLP, \n",
    "organizations can gain valuable insights into customer thoughts, behaviors, patterns, and preferences. \n",
    "Consequently, NLP is being used in many industries to uncover opportunities for better customer experiences, \n",
    "increased efficiency in operations, improved decision-making, and advanced analytics.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236caee4",
   "metadata": {},
   "source": [
    "Use the corpus above to generate embeddings using the following techniques:\n",
    "1. `TFiDF`\n",
    "2.  `Bag of Words`\n",
    "3. `CBOW`\n",
    "4. `SkipGram`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5678cd1",
   "metadata": {},
   "source": [
    "Before we generate embeddings, we'll leverage the function from the last exerciess notebook to clean up the corpus by tokenizing, removing stop words, and lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e911d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "from nltk.stem import PorterStemmer\n",
    "from functools import reduce\n",
    "\n",
    "# This function will remove stopwords\n",
    "def cleanupDoc(s):\n",
    "     stopset = set(stopwords.words('english'))\n",
    "     cleanup = \" \".join(filter(lambda word: word not in stopset, s.split()))\n",
    "     return cleanup\n",
    "\n",
    "# This function will use the `PorterStemmer` to reduce words to their stem\n",
    "def stemmer_(s):\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    port_stemmer = PorterStemmer()\n",
    "    cleanup = reduce(lambda x, y: x + \" \" + port_stemmer.stem(y), tokens, \"\")\n",
    "    return cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a3c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions defined above to clean data \n",
    "cleaned_text = cleanupDoc(corpus)\n",
    "cleaner_text = stemmer_(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcfdc3",
   "metadata": {},
   "source": [
    "## Using Inverse Term Frequency to Generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f931d",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, fill in the code below to apply the `TF-iDF` vecotrizer from the `sklearn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05729e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x60 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 60 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate an instace of the TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "# Fit the vectorizer to corputs \n",
    "fitted_vectorizer = vectorizer.fit([cleaner_text])\n",
    "\n",
    "# Transform the corpuse using the fit vectorizer \n",
    "X = vectorizer.transform([cleaner_text])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a410afd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['advanc', 'analyt', 'analyz', 'becom', 'behavior', 'better',\n",
       "       'busi', 'by', 'compani', 'consequ', 'custom', 'decis', 'decision',\n",
       "       'deriv', 'design', 'determin', 'effici', 'email', 'experi',\n",
       "       'extract', 'feedback', 'for', 'gain', 'improv', 'increas',\n",
       "       'industri', 'influenc', 'insight', 'instanc', 'integr', 'intent',\n",
       "       'languag', 'leverag', 'mak', 'mani', 'may', 'mean', 'meaning',\n",
       "       'natur', 'nlp', 'oper', 'opportun', 'organ', 'part', 'pattern',\n",
       "       'power', 'prefer', 'process', 'product', 'rapidli', 'respons',\n",
       "       'review', 'sentiment', 'text', 'the', 'thought', 'uncov',\n",
       "       'understand', 'use', 'valuabl'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf3fa8",
   "metadata": {},
   "source": [
    "We can use the results of the fitted vectorizer transformed into a `DataFrame` to take a quick look at which words have the highest term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594df8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TF-IDF\n",
      "nlp       0.464238\n",
      "custom    0.371391\n",
      "use       0.278543\n",
      "mani      0.185695\n",
      "insight   0.185695\n",
      "process   0.185695\n",
      "advanc    0.092848\n",
      "organ     0.092848\n",
      "opportun  0.092848\n",
      "oper      0.092848\n"
     ]
    }
   ],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Convert the sparse matrix into a Pandas DataFrame for quick analysis\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X[0].T.todense(), \n",
    "                  index = vectorizer.get_feature_names_out(), \n",
    "                  columns=[\"TF-IDF\"]\n",
    "                 )\n",
    "print(df.sort_values(\"TF-IDF\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28171b",
   "metadata": {},
   "source": [
    "## Using Bag of Words to Generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e6d16",
   "metadata": {},
   "source": [
    "As an alternative to `TF-iDF` the `Continuous Bag of Words` algorithm can be applied to generate counting based embeddings. In the cells below, please fill in the missing code to generate `CBOW` embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d545bcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x60 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 60 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate Count Vectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit to the corpus \n",
    "fitted_vectorizer = vectorizer.fit([cleaner_text])\n",
    "\n",
    "# Transform using fitted vectorizer \n",
    "X = fitted_vectorizer.transform([cleaner_text])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87fe883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['advanc', 'analyt', 'analyz', 'becom', 'behavior', 'better',\n",
       "       'busi', 'by', 'compani', 'consequ', 'custom', 'decis', 'decision',\n",
       "       'deriv', 'design', 'determin', 'effici', 'email', 'experi',\n",
       "       'extract', 'feedback', 'for', 'gain', 'improv', 'increas',\n",
       "       'industri', 'influenc', 'insight', 'instanc', 'integr', 'intent',\n",
       "       'languag', 'leverag', 'mak', 'mani', 'may', 'mean', 'meaning',\n",
       "       'natur', 'nlp', 'oper', 'opportun', 'organ', 'part', 'pattern',\n",
       "       'power', 'prefer', 'process', 'product', 'rapidli', 'respons',\n",
       "       'review', 'sentiment', 'text', 'the', 'thought', 'uncov',\n",
       "       'understand', 'use', 'valuabl'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b23b8",
   "metadata": {},
   "source": [
    "We can use the results of the fitted vectorizer transformed into a `DataFrame` to take a quick look at which words have the highest `Bag of Words` score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e0715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Bag of Words\n",
      "nlp                  5\n",
      "custom               4\n",
      "use                  3\n",
      "mani                 2\n",
      "insight              2\n",
      "process              2\n",
      "advanc               1\n",
      "organ                1\n",
      "opportun             1\n",
      "oper                 1\n"
     ]
    }
   ],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Convert the sparse matrix into a Pandas DataFrame for later modeling \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X[0].T.todense(), \n",
    "                  index = vectorizer.get_feature_names_out(), \n",
    "                  columns=[\"Bag of Words\"]\n",
    "                 )\n",
    "print(df.sort_values(\"Bag of Words\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd4dd5",
   "metadata": {},
   "source": [
    "## Using Word2Vec to Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63b5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Train Word2Vec using CBOW\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Sentence and word tokenize cleaned text \n",
    "data = []\n",
    "\n",
    "for i in sent_tokenize(cleaner_text):\n",
    "    temp = []\n",
    "    \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "        \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4104e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "cbow = Word2Vec(data, vector_size=100, min_count=1,sg = 0, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85dbf7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Train Word2Vec using Skip Gram\n",
    "skip_gram = Word2Vec(data, vector_size=100, min_count=1, sg=1, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0f299",
   "metadata": {},
   "source": [
    "Now that we've built the two different models, let's compare the two by looking at similarity scores for a set of words across the two different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59342739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between `nlp` and `custom` using CBOW Model: -0.013326079584658146\n",
      "Cosine similarity between `nlp` and `custom` using Skip Gram Model: -0.013088701292872429\n"
     ]
    }
   ],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Calculate similarities \n",
    "cbow_similarity = cbow.wv.similarity(\"nlp\", \"custom\")\n",
    "skip_gram_similarity = skip_gram.wv.similarity(\"nlp\", \"custom\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Cosine similarity between `nlp` and `custom` using CBOW Model: {cbow_similarity}\")\n",
    "print(f\"Cosine similarity between `nlp` and `custom` using Skip Gram Model: {skip_gram_similarity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
