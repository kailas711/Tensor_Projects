{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0eac062",
   "metadata": {},
   "source": [
    "# Large Language Models and Transfer Learning \n",
    "\t\n",
    "The aforementioned deep learning models all vary in architecture but they share a rather high level of complexity and a high training cost. Building some of these models from scratch requires a massive amount of labeled data,  an enormous number of training resources and money to pay for those resources. Depending on several factors, those required resources might not be available. In lieu of these resources, the industry leans on **transfer learning**  using pre-trained **large language models** to leverage these powerful neural networks for NLP. Transfer learning is a machine learning technique where a model that’s trained on one task is re-purposed on a second related task. Transfer learning aims to reduce the amount of training data and resources required for a new task by relying on features and weights learned from a previously trained model. This technique also allows for models’ weights to be fine-tuned depending on the similarity of the task the model was originally trained on and the new task at hand. While fine-tuning can be useful to tailor LLMs to specific tasks, they still require a decent amount of labeled data. With the rise of transfer learning, more and more LLMs have been rolled out by research institutions to be used by the masses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed3c62",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "The first common family of LLMs is the **BERT** family. **Bidirectional Encoder Representations from Transformers** (BERT) is a family of models. This family was originally developed by Google and is a seq2seq model with multiple self-attention layers. BERT and BERT adjacent models can be used to accurately predict the meaning of words or phrases in a sentence as a means towards understanding the relationship between them. BERT is commonly used for tasks like text classification, question answering, and sentiment analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d1cde",
   "metadata": {},
   "source": [
    "### Huggingface \n",
    "[Huggingface](https://huggingface.co/) is an open source commuinty where pre-trained models are published for use. BERT and thousands of other models are avaiable for free via Huggingface's `transformers` library. The library supports both `Tensorflow` APIs and `PyTorch` APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c48edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaila\\Projects\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15684dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'out', 'the', 'bert', 'token', '##izer', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the tokenizer using the Bert-Base-Uncased pretrained weights \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Test is out \n",
    "test_string = \"Let's test out the Bert Tokenizer!!\"\n",
    "\n",
    "# Apply the tokenizer \n",
    "output = tokenizer.tokenize(test_string)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a57f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2292, 1005, 1055, 3231, 2041, 1996, 14324, 19204, 17629, 999, 999]\n"
     ]
    }
   ],
   "source": [
    "# Convert to the indices using the BERT Tokenizer \n",
    "bert_output = tokenizer.convert_tokens_to_ids(output)\n",
    "print(bert_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc27825",
   "metadata": {},
   "source": [
    "## ELMo\t\n",
    "**ELMo** or **Embeddings from Language Models** is another common large language model that uses context from both the left and the right to generate accurate word representations. It was developed by the Allen Institute for Artificial Intelligence and its output is commonly used as input to other NLP models. Keep in mind that the output is embeddings, the numeric representations of words, that convey contextual information of the surrounding words. As a result, EMLo is useful for tasks that require a deep understanding of complex language like text generation or machine translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f2f09",
   "metadata": {},
   "source": [
    "### TensorFlow Hub\n",
    "As an alternative to `Huggingface`, [TensorFlow Hub](https://www.tensorflow.org/hub) is a repository of pretrained models that can be used, fine-tuned, and deployed. However, unlike `Huggingface`, `TensorFlow Hub` is only compatible with the `TensorFlow` APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16218f80",
   "metadata": {},
   "source": [
    "## GPT\t\n",
    "The next family of models has recently taken over the spotlight - the **GPT** or **Generative Pre-trained Transformer** family of models developed by Open AI. As stated in the name, GPT models are generative and are trained using a self-supervised approach. This means they are trained on massive amounts of unlabeled text data using the transformer architecture. Because they’re trained on so much data and can capture long term dependencies, the GPT family of models are well suited for tasks like question answering and text generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb7bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Let's test the GPT2 tokenizer!!\"\n",
    "gpt2_input = tokenizer(example_text, padding=\"max_length\", max_length=10, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f24539",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lables dictionary\n",
    "labels = {\"0\":0,\"1\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bd066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Torch dataset class \n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Constructor \n",
    "        \n",
    "        @params:\n",
    "        df: pd.DataFrame\n",
    "        \n",
    "        @returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.labels = [labels[label] for label in df['spam']]\n",
    "        self.texts = [tokenizer(text,\n",
    "                                padding='max_length',\n",
    "                                max_length=128,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "        \n",
    "    def classes(self):\n",
    "        \"\"\"\n",
    "        Function to get labes \n",
    "        \n",
    "        @params:\n",
    "        \n",
    "        @returns:\n",
    "        self.labels: labels of target in pd.DataFrame\n",
    "        \"\"\"\n",
    "        return self.labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the lenth of labels \n",
    "        \n",
    "        @params:\n",
    "        \n",
    "        @returns:\n",
    "        len(self.labels): int, number of distinct label classes\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        \"\"\"\n",
    "        Gets arrray of labes for each batch\n",
    "        \n",
    "        @params:\n",
    "        idx: int, index \n",
    "        \n",
    "        @returns \n",
    "        np.array(self.labels[idx]: np.array, labels given index)\n",
    "        \"\"\"\n",
    "        # Get a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "    \n",
    "    def get_batch_texts(self, idx):\n",
    "        \"\"\"\n",
    "        Get batch text inputs \n",
    "        \n",
    "        @params:\n",
    "        idx: int, index\n",
    "        \n",
    "        @returns:\n",
    "        self.texts[idx]: str, text associated with index\n",
    "        \"\"\"\n",
    "        # Get a batch of inputs\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in emails data\n",
    "df = pd.read_csv(\"data/emails.csv\")\n",
    "df[\"spam\"] = df[\"spam\"].astype(\"str\")\n",
    "\n",
    "# Split data\n",
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=35),\n",
    "                                     [int(0.8*len(df)), int(0.9*len(df))])\n",
    "\n",
    "print(f\"Rows in training set: {len(df_train)}, Rows in validation set: {len(df_val)}, Rows in test set: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT2SequenceClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT2 Sequence classifier class\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int, num_classes:int ,max_seq_len:int, gpt_model_name:str):\n",
    "        \"\"\"\n",
    "        SimpleGPT2SequenceClassifier constructor \n",
    "        \n",
    "        @params:\n",
    "        hidden_size: int, \n",
    "        num_classes: int, \n",
    "        max_seq_len: int, \n",
    "        gpt_model_name: str\n",
    "\n",
    "        @returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        super(SimpleGPT2SequenceClassifier,self).__init__()\n",
    "        self.gpt2model = GPT2Model.from_pretrained(gpt_model_name)\n",
    "        self.fc1 = nn.Linear(hidden_size*max_seq_len, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, input_id, mask):\n",
    "        \"\"\"\n",
    "        Forward training \n",
    "        \n",
    "        @params:\n",
    "        input_id:\n",
    "        mask:\n",
    "        \n",
    "        @returns:\n",
    "        linear_output\n",
    "        \"\"\"\n",
    "        gpt_out, _ = self.gpt2model(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        batch_size = gpt_out.shape[0]\n",
    "        linear_output = self.fc1(gpt_out.view(batch_size,-1))\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    Function that fine tunes GPT2 model using the Email Spam data \n",
    "    \n",
    "    @params:\n",
    "    model:\n",
    "    train_data: pd.DataFrame,\n",
    "    val_data: pd.DataFrame,\n",
    "    learning_rate: float, \n",
    "    epochs: int\n",
    "    \n",
    "    @returns:\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            \n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            acc = (output.argmax(dim=1)==train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1)==val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "                \n",
    "            print(\n",
    "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train/len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}\")\n",
    "            \n",
    "EPOCHS = 1\n",
    "model = SimpleGPT2SequenceClassifier(hidden_size=768, num_classes=2, max_seq_len=128, gpt_model_name=\"gpt2\")\n",
    "LR = 1e-5\n",
    "\n",
    "# Train model for a single epoch\n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbe902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    \"\"\"\n",
    "    Evalute the model performance \n",
    "    \n",
    "    @params:\n",
    "    model: SimpleGPT2SequenceClassifier, trained GPT2 classifier\n",
    "    test_data: pd.Dataframe\n",
    "    \n",
    "    @returns:\n",
    "    true_labels: List[int], true labels\n",
    "    predictions_labels: List[int], predicted lables \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "        \n",
    "    # Tracking variables\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    \n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "            \n",
    "            # add original labels\n",
    "            true_labels += test_label.cpu().numpy().flatten().tolist()\n",
    "            # get predicitons to list\n",
    "            predictions_labels += output.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    return true_labels, predictions_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afa5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with accuracy \n",
    "true_labels, pred_labels = evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix of results\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "cm = confusion_matrix(y_true=true_labels, y_pred=pred_labels, labels=range(len(labels)), normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(labels.keys()))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73b205",
   "metadata": {},
   "source": [
    "## ULMFiT\t\n",
    "The final model family is the **ULMFiT** or the **Universal Language Model Fine-tuning** family of models. These models are built by fast.ai and are trained with a transfer learning approach. While they leverage the same transformer architecture mentioned previously, the transfer learning approach ensures these models are flexible to various tasks outside of the one it was initially trained on. They’re able to better adapt to new tasks using a small amount of data, while some other LLMs require massive amounts of data to alternate tasks.  ULMFiT models are well suited for tasks like text classification, sentiment analysis, and language generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bceccd2",
   "metadata": {},
   "source": [
    "In order to use the ULMFiT model, we'll need to install and import the `fastai` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f799f7da",
   "metadata": {},
   "source": [
    "## Machine Learning vs. Deep Learning vs. LLMs for NLP \n",
    "\n",
    "While it’s important to distinguish the difference between techniques and pretrained models, this difference is also important to map proper techniques to proper tasks. When deciding on which model to use, whether traditional machine learning, deep learning, or a pretrained model, there are few guiding principles to consider. The first is accessibility to labeled data. Many of these models require large amounts of data to fine tune billions of parameters. Quality, labeled data is hard to come by and could be a limiting factor for building a DL model from scratch. Another factor to consider is cost, these large language models, while powerful, can be expensive to fine tune depending on the task especially considering they might need pricey GPU hardware. A third principle to consider is any regulatory or security requirements with sensitive data. Regulatory requirements can impact the needs for model interpretability which might be hard to track for some NLP models. Amongst these principles and more, the approach to NLP should be the same as any other in ML, iterative. Start with a simple, baseline mode and add complexity to measure model performance against the baseline. This could mean starting with a simple machine learning model as a baseline and iterating all the way up to an LLM to achieve the best performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbeecf",
   "metadata": {},
   "source": [
    "## A Quick Note on Challenges in NLP \n",
    "Like any other field in machine learning, natural language processing comes with its own unique set of challenges. One of the more prominent challenges in NLP is data sparsity. In most cases, tasks are dealing with large vocabularies, but it’s near impossible to have examples of all possible language outcomes. To combat this, data scientists rely on proper representation in the corpus however this proper representation can be hard to come by and also manually intensive to verify. Somewhat related, the unstructured nature of NLP data proves to be a challenge as well. Language is fluid and can be inconsistent depending on tone. As a result models can pick up on inaccurate context depending on the corpus. A third prominent challenge in NLP, especially pertaining to LLMs, is the domain adaptation of pretrained models. If the task at hand is significantly different than the task a LLM was trained on, lots of data and compute are required to fine tune models to fit certain problem domains. Sometimes the luxury of compute and large amounts of labeled data aren’t accessible. Like any other machine learning domain, NLP has its challenges, most of those challenges in NLP pertain to the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542635d",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "\n",
    "To review, NLP is a field in machine learning that focuses on understanding and processing natural human language. NLP involves techniques ranging from text processing and sentiment analysis to machine translation and question answering. NLP models can use shallow or deep learning-based architectures. Large, pretrained language models like BERT, ELMo, GPT, and ULMFiT, are used to accurately predict the meaning of words or phrases in a sentence and to understand the relationship between them. NLP models are used for a variety of tasks, such as text classification, question answering, sentiment analysis, and language generation. Like any subset in machine learning there are challenges, in NLP most of those challenges trace back to the data. However, when executed well, NLP models can have a massive impact on everyday life and innovation. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
